{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Unaugmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_current_images = 75 #training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images to train:  52.5\n"
     ]
    }
   ],
   "source": [
    "train_per = 0.7 #70% of data to train\n",
    "images_to_train = no_of_current_images * train_per\n",
    "print(\"number of images to train: \",images_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images to test:  15.0\n"
     ]
    }
   ],
   "source": [
    "test_per = 0.20 #20 of data to train\n",
    "images_to_test = no_of_current_images * test_per\n",
    "print(\"number of images to test: \",images_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images to test:  7.5\n"
     ]
    }
   ],
   "source": [
    "val_per = 0.1 #10% of data to train\n",
    "images_to_val = no_of_current_images * val_per\n",
    "print(\"number of images to test: \",images_to_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5754523026315789 0.7554824561403508 0.280016447368421 0.45394736842105254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data', 'train','labels','1caf4a00-9670844a-31cc-11ee-a456-e68a90e86460.txt'), 'r') as f:\n",
    "    content = f.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tesitng yolo with a random img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.predict(source = 'data/test/images/bd29bc18-e385a116-1fed-4fd3-acba-29db1659da3a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.149 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.147 🚀 Python-3.11.4 torch-2.0.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/rei/runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/rei/runs/detect/train3', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/rei/ml/araya/yolov8_test/data/train/labels... 52 images, 1 backgrounds, 0 corrupt: 100%|██████████| 52/52 [00:00<00:00, 1496.83it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/rei/ml/araya/yolov8_test/data/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/rei/ml/araya/yolov8_test/data/val/labels... 8 images, 0 backgrounds, 0 corrupt: 100%|██████████| 8/8 [00:00<00:00, 2645.41it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/rei/ml/araya/yolov8_test/data/val/labels.cache\n",
      "Plotting labels to /Users/rei/runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/rei/runs/detect/train3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50         0G      1.384      3.386      1.678         11        640: 100%|██████████| 4/4 [00:41<00:00, 10.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8    0.00333          1      0.267      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50         0G      1.262      2.943      1.474         11        640: 100%|██████████| 4/4 [00:36<00:00,  9.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "                   all          8          8    0.00333          1        0.9      0.615\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50         0G      1.153      2.399      1.301         15        640: 100%|██████████| 4/4 [00:36<00:00,  9.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8    0.00333          1      0.962      0.591\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50         0G      1.227      1.767      1.299         16        640: 100%|██████████| 4/4 [00:37<00:00,  9.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8          1       0.35      0.995      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50         0G       1.05      1.695      1.158          7        640: 100%|██████████| 4/4 [00:36<00:00,  9.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8      0.611          1      0.995       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/50         0G     0.9809      1.711      1.139         16        640: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.835          1      0.995      0.657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/50         0G      1.177      1.706      1.183         11        640: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "                   all          8          8    0.00626          1      0.982      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/50         0G      1.191      1.522       1.15         31        640: 100%|██████████| 4/4 [00:36<00:00,  9.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8    0.00983          1      0.955      0.724\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/50         0G      1.049      1.356      1.176         11        640: 100%|██████████| 4/4 [00:38<00:00,  9.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8    0.00824          1      0.784      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/50         0G       1.08      1.488      1.143          8        640: 100%|██████████| 4/4 [00:36<00:00,  9.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8    0.00333          1      0.796      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/50         0G      1.265       1.32       1.16         10        640: 100%|██████████| 4/4 [00:36<00:00,  9.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8     0.0119          1       0.88      0.626\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/50         0G      1.051      1.379        1.1          6        640: 100%|██████████| 4/4 [00:38<00:00,  9.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8    0.00333          1      0.879      0.588\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/50         0G      1.141      1.399       1.18         10        640: 100%|██████████| 4/4 [00:38<00:00,  9.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8    0.00333          1      0.801      0.532\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/50         0G      1.165      1.355      1.186          9        640: 100%|██████████| 4/4 [00:37<00:00,  9.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "                   all          8          8     0.0109          1      0.814      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/50         0G      1.059      1.495       1.21          7        640: 100%|██████████| 4/4 [00:37<00:00,  9.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.453       0.88      0.569\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/50         0G      1.157      1.249       1.21         12        640: 100%|██████████| 4/4 [00:37<00:00,  9.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "                   all          8          8          1      0.752      0.917      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/50         0G      1.082      1.324      1.152         15        640: 100%|██████████| 4/4 [00:37<00:00,  9.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.844      0.949      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/50         0G      1.048      1.336      1.124          7        640: 100%|██████████| 4/4 [00:39<00:00,  9.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "                   all          8          8      0.864      0.797      0.915      0.546\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/50         0G      1.121      1.203      1.148         23        640: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "                   all          8          8      0.862      0.875      0.927      0.602\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/50         0G      1.039        1.2      1.087         11        640: 100%|██████████| 4/4 [00:38<00:00,  9.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8      0.978      0.875      0.955      0.672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      21/50         0G      1.021      1.139      1.096         10        640: 100%|██████████| 4/4 [00:39<00:00,  9.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "                   all          8          8      0.999      0.875      0.962      0.678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      22/50         0G      1.041      1.111      1.148         11        640: 100%|██████████| 4/4 [00:36<00:00,  9.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.987      0.995      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      23/50         0G      1.232      1.201      1.219         20        640: 100%|██████████| 4/4 [00:37<00:00,  9.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8       0.99      0.875      0.982      0.726\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      24/50         0G      1.076      1.295      1.149          6        640: 100%|██████████| 4/4 [00:44<00:00, 11.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8          1      0.868      0.982      0.733\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      25/50         0G      1.007      1.152      1.085         12        640: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.986      0.875      0.982      0.708\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      26/50         0G      1.045      1.146      1.093         16        640: 100%|██████████| 4/4 [00:39<00:00, 10.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "                   all          8          8          1      0.994      0.995      0.757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      27/50         0G      1.012       1.11      1.067         15        640: 100%|██████████| 4/4 [00:38<00:00,  9.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "                   all          8          8      0.888      0.991      0.912      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      28/50         0G      0.929      1.008       1.08         13        640: 100%|██████████| 4/4 [00:37<00:00,  9.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "                   all          8          8          1      0.989      0.995      0.793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      29/50         0G     0.9518      1.014      1.061          8        640: 100%|██████████| 4/4 [00:41<00:00, 10.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.991      0.995      0.763\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      30/50         0G      1.029      1.216      1.082         25        640: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.991      0.995      0.722\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      31/50         0G      1.032      1.009      1.146         16        640: 100%|██████████| 4/4 [00:36<00:00,  9.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.887      0.984      0.926      0.617\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      32/50         0G     0.8524      1.033      1.053          6        640: 100%|██████████| 4/4 [00:36<00:00,  9.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8      0.884          1      0.954      0.645\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      33/50         0G      1.011      1.035      1.104         11        640: 100%|██████████| 4/4 [00:39<00:00,  9.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8      0.988          1      0.995      0.628\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      34/50         0G     0.9033     0.9358      1.081         16        640: 100%|██████████| 4/4 [00:36<00:00,  9.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.882          1      0.982      0.686\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      35/50         0G     0.9664     0.9631      1.083         16        640: 100%|██████████| 4/4 [00:36<00:00,  9.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "                   all          8          8      0.983          1      0.995      0.678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      36/50         0G     0.9176     0.9991      1.085         11        640: 100%|██████████| 4/4 [00:37<00:00,  9.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.994      0.995      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      37/50         0G     0.9749     0.9901      1.083          9        640: 100%|██████████| 4/4 [00:42<00:00, 10.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8          1       0.99      0.995      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      38/50         0G      0.837     0.9195      1.016         12        640: 100%|██████████| 4/4 [00:36<00:00,  9.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8      0.981      0.875      0.982      0.726\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      39/50         0G     0.9317     0.9282      1.094         11        640: 100%|██████████| 4/4 [00:37<00:00,  9.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8          1      0.982      0.995      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      40/50         0G     0.7514     0.8229     0.9965         10        640: 100%|██████████| 4/4 [00:36<00:00,  9.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "                   all          8          8      0.991          1      0.995       0.69\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      41/50         0G     0.8204      1.134     0.9697         10        640: 100%|██████████| 4/4 [00:36<00:00,  9.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.996          1      0.995      0.657\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      42/50         0G     0.7029      1.068     0.9336          4        640: 100%|██████████| 4/4 [00:37<00:00,  9.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8      0.995          1      0.995      0.665\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      43/50         0G      0.774      1.039     0.9373          6        640: 100%|██████████| 4/4 [00:35<00:00,  8.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.995          1      0.995      0.675\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      44/50         0G     0.6832       1.08     0.9471          4        640: 100%|██████████| 4/4 [00:35<00:00,  8.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.995          1      0.995      0.719\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      45/50         0G     0.7406       1.07     0.9372          4        640: 100%|██████████| 4/4 [00:37<00:00,  9.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "                   all          8          8      0.994          1      0.995      0.737\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      46/50         0G     0.8105     0.9984      0.983          4        640: 100%|██████████| 4/4 [00:35<00:00,  8.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.993          1      0.995      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      47/50         0G     0.6717      1.035     0.9721          3        640: 100%|██████████| 4/4 [00:37<00:00,  9.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "                   all          8          8      0.993          1      0.995      0.735\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      48/50         0G     0.6488     0.9517     0.9423          4        640: 100%|██████████| 4/4 [00:37<00:00,  9.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "                   all          8          8      0.992          1      0.995      0.745\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      49/50         0G     0.6678     0.9043     0.9046          4        640: 100%|██████████| 4/4 [00:35<00:00,  8.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "                   all          8          8      0.992          1      0.995      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      50/50         0G     0.6363     0.8925     0.9111          4        640: 100%|██████████| 4/4 [00:35<00:00,  8.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "                   all          8          8      0.992          1      0.995      0.706\n",
      "\n",
      "50 epochs completed in 0.540 hours.\n",
      "Optimizer stripped from /Users/rei/runs/detect/train3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /Users/rei/runs/detect/train3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /Users/rei/runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.147 🚀 Python-3.11.4 torch-2.0.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "                   all          8          8          1      0.989      0.995      0.793\n",
      "Speed: 0.9ms preprocess, 102.2ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/rei/runs/detect/train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO()\n",
    "\n",
    "model.train(data=\"data.yaml\", epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING ⚠️ 'ultralytics.yolo.cfg' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.cfg' instead.\n",
      "Ultralytics YOLOv8.0.147 🚀 Python-3.11.4 torch-2.0.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/rei/ml/araya/yolov8_test/data/val/labels.cache... 8 images,\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          8          8          1      0.989      0.995      0.793\n",
      "Speed: 0.8ms preprocess, 100.9ms inference, 0.0ms loss, 3.7ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/rei/runs/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "!yolo task=detect mode=val model=\"best.pt\" data = data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/15 /Users/rei/ml/araya/yolov8_test/data/test/images/bd29bc18-e385a116-1fed-4fd3-acba-29db1659da3a.jpg: 352x640 4 persons, 179.0ms\n",
      "image 2/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c1f4b19e-3c3c8c68-10e2-11ee-ba5f-e68a90e86460.jpg: 384x640 2 persons, 149.9ms\n",
      "image 3/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c61c9f56-98119b72-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 118.1ms\n",
      "image 4/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c6fd24b8-d9ba4b72-0ac9-49ac-95a6-d4553bef8f46.jpg: 448x640 1 person, 112.8ms\n",
      "image 5/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c96411eb-2cb38da0-10e2-11ee-ba5f-e68a90e86460.jpg: 384x640 1 person, 112.4ms\n",
      "image 6/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c9ebfc2e-9a550dba-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 114.0ms\n",
      "image 7/15 /Users/rei/ml/araya/yolov8_test/data/test/images/cdec6dd7-8f4a2162-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 125.3ms\n",
      "image 8/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d17d0693-95ce0eb8-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 325.6ms\n",
      "image 9/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d3ab0bde-0308a25f-fd01-4ccc-9871-a948fddfe77d.jpg: 288x640 1 person, 102.2ms\n",
      "image 10/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d578e605-94322ba2-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 127.3ms\n",
      "image 11/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d7e84f41-92ce080a-39db-4ceb-85b5-f30997cc16f0.jpg: 416x640 1 person, 171.0ms\n",
      "image 12/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d904e901-9197fa70-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 125.3ms\n",
      "image 13/15 /Users/rei/ml/araya/yolov8_test/data/test/images/dc07c56d-9a03b168-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 125.9ms\n",
      "image 14/15 /Users/rei/ml/araya/yolov8_test/data/test/images/e4fa758d-99611278-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 115.3ms\n",
      "image 15/15 /Users/rei/ml/araya/yolov8_test/data/test/images/e723a8b2-923ad2ea-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 129.6ms\n",
      "Speed: 3.0ms preprocess, 142.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[208, 196, 192],\n",
       "         [208, 196, 192],\n",
       "         [207, 195, 191],\n",
       "         ...,\n",
       "         [191, 179, 179],\n",
       "         [188, 176, 176],\n",
       "         [183, 171, 171]],\n",
       " \n",
       "        [[207, 195, 191],\n",
       "         [207, 195, 191],\n",
       "         [206, 194, 190],\n",
       "         ...,\n",
       "         [190, 178, 178],\n",
       "         [190, 178, 178],\n",
       "         [186, 174, 174]],\n",
       " \n",
       "        [[205, 193, 189],\n",
       "         [205, 193, 189],\n",
       "         [205, 193, 189],\n",
       "         ...,\n",
       "         [189, 177, 177],\n",
       "         [191, 179, 179],\n",
       "         [188, 176, 176]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[176, 164, 160],\n",
       "         [172, 160, 156],\n",
       "         [175, 163, 159],\n",
       "         ...,\n",
       "         [123, 109, 113],\n",
       "         [125, 111, 115],\n",
       "         [126, 112, 116]],\n",
       " \n",
       "        [[168, 156, 152],\n",
       "         [173, 161, 157],\n",
       "         [170, 158, 154],\n",
       "         ...,\n",
       "         [124, 110, 114],\n",
       "         [121, 107, 111],\n",
       "         [119, 105, 109]],\n",
       " \n",
       "        [[168, 156, 152],\n",
       "         [173, 161, 157],\n",
       "         [170, 158, 154],\n",
       "         ...,\n",
       "         [124, 110, 114],\n",
       "         [121, 107, 111],\n",
       "         [119, 105, 109]]], dtype=uint8)\n",
       " orig_shape: (637, 1200)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/bd29bc18-e385a116-1fed-4fd3-acba-29db1659da3a.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 12.736082077026367, 'inference': 178.9710521697998, 'postprocess': 6.916284561157227},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[ 75,  93, 100],\n",
       "         [ 68,  86,  93],\n",
       "         [ 73,  91,  98],\n",
       "         ...,\n",
       "         [ 35,  48,  62],\n",
       "         [ 41,  52,  66],\n",
       "         [ 45,  56,  70]],\n",
       " \n",
       "        [[ 71,  89,  96],\n",
       "         [ 65,  83,  90],\n",
       "         [ 69,  87,  94],\n",
       "         ...,\n",
       "         [ 38,  51,  65],\n",
       "         [ 40,  51,  65],\n",
       "         [ 38,  49,  63]],\n",
       " \n",
       "        [[ 69,  87,  94],\n",
       "         [ 67,  85,  92],\n",
       "         [ 69,  87,  94],\n",
       "         ...,\n",
       "         [ 42,  55,  69],\n",
       "         [ 45,  58,  72],\n",
       "         [ 41,  54,  68]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 58,  75,  84],\n",
       "         [ 56,  73,  82],\n",
       "         [ 55,  72,  81],\n",
       "         ...,\n",
       "         [ 42,  51,  61],\n",
       "         [ 45,  54,  63],\n",
       "         [ 45,  54,  63]],\n",
       " \n",
       "        [[ 57,  74,  83],\n",
       "         [ 60,  77,  86],\n",
       "         [ 57,  74,  83],\n",
       "         ...,\n",
       "         [ 40,  49,  59],\n",
       "         [ 42,  51,  60],\n",
       "         [ 47,  56,  65]],\n",
       " \n",
       "        [[ 60,  77,  86],\n",
       "         [ 61,  78,  87],\n",
       "         [ 51,  68,  77],\n",
       "         ...,\n",
       "         [ 41,  50,  60],\n",
       "         [ 47,  56,  65],\n",
       "         [ 54,  63,  72]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/c1f4b19e-3c3c8c68-10e2-11ee-ba5f-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 3.705739974975586, 'inference': 149.9030590057373, 'postprocess': 0.8330345153808594},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[177, 193, 200],\n",
       "         [176, 192, 199],\n",
       "         [177, 193, 200],\n",
       "         ...,\n",
       "         [152, 165, 167],\n",
       "         [151, 164, 166],\n",
       "         [150, 163, 165]],\n",
       " \n",
       "        [[177, 193, 200],\n",
       "         [176, 192, 199],\n",
       "         [177, 193, 200],\n",
       "         ...,\n",
       "         [150, 163, 165],\n",
       "         [150, 163, 165],\n",
       "         [149, 162, 164]],\n",
       " \n",
       "        [[176, 192, 199],\n",
       "         [174, 190, 197],\n",
       "         [177, 193, 200],\n",
       "         ...,\n",
       "         [151, 164, 166],\n",
       "         [151, 164, 166],\n",
       "         [151, 164, 166]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 69,  73,  92],\n",
       "         [ 68,  72,  91],\n",
       "         [ 71,  75,  94],\n",
       "         ...,\n",
       "         [ 31,  35,  36],\n",
       "         [ 32,  32,  32],\n",
       "         [ 45,  45,  45]],\n",
       " \n",
       "        [[ 63,  69,  88],\n",
       "         [ 62,  68,  87],\n",
       "         [ 66,  72,  91],\n",
       "         ...,\n",
       "         [ 25,  29,  30],\n",
       "         [ 28,  28,  28],\n",
       "         [ 37,  37,  37]],\n",
       " \n",
       "        [[ 64,  70,  89],\n",
       "         [ 66,  72,  91],\n",
       "         [ 61,  67,  86],\n",
       "         ...,\n",
       "         [ 28,  32,  33],\n",
       "         [ 38,  38,  38],\n",
       "         [ 35,  35,  35]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/c61c9f56-98119b72-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.6481151580810547, 'inference': 118.13807487487793, 'postprocess': 0.7128715515136719},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[198, 199, 195],\n",
       "         [196, 198, 192],\n",
       "         [188, 191, 182],\n",
       "         ...,\n",
       "         [148, 155, 152],\n",
       "         [169, 176, 173],\n",
       "         [159, 166, 163]],\n",
       " \n",
       "        [[186, 187, 183],\n",
       "         [201, 203, 197],\n",
       "         [190, 192, 186],\n",
       "         ...,\n",
       "         [134, 139, 137],\n",
       "         [148, 153, 151],\n",
       "         [157, 162, 160]],\n",
       " \n",
       "        [[193, 195, 189],\n",
       "         [195, 197, 191],\n",
       "         [197, 200, 191],\n",
       "         ...,\n",
       "         [165, 172, 169],\n",
       "         [146, 153, 150],\n",
       "         [132, 139, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[247, 245, 244],\n",
       "         [247, 245, 244],\n",
       "         [247, 245, 244],\n",
       "         ...,\n",
       "         [184, 212, 236],\n",
       "         [194, 219, 239],\n",
       "         [197, 221, 239]],\n",
       " \n",
       "        [[245, 243, 242],\n",
       "         [246, 244, 243],\n",
       "         [248, 246, 245],\n",
       "         ...,\n",
       "         [191, 216, 236],\n",
       "         [189, 216, 236],\n",
       "         [193, 219, 236]],\n",
       " \n",
       "        [[245, 243, 242],\n",
       "         [247, 245, 244],\n",
       "         [248, 246, 245],\n",
       "         ...,\n",
       "         [192, 217, 237],\n",
       "         [188, 215, 235],\n",
       "         [192, 217, 237]]], dtype=uint8)\n",
       " orig_shape: (2666, 4000)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/c6fd24b8-d9ba4b72-0ac9-49ac-95a6-d4553bef8f46.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 3.709077835083008, 'inference': 112.84208297729492, 'postprocess': 0.49614906311035156},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[44, 66, 71],\n",
       "         [45, 67, 72],\n",
       "         [42, 64, 69],\n",
       "         ...,\n",
       "         [27, 45, 56],\n",
       "         [23, 41, 52],\n",
       "         [21, 39, 50]],\n",
       " \n",
       "        [[45, 67, 72],\n",
       "         [43, 65, 70],\n",
       "         [37, 59, 64],\n",
       "         ...,\n",
       "         [23, 41, 52],\n",
       "         [24, 42, 53],\n",
       "         [23, 41, 52]],\n",
       " \n",
       "        [[42, 64, 69],\n",
       "         [45, 67, 72],\n",
       "         [43, 65, 70],\n",
       "         ...,\n",
       "         [19, 39, 50],\n",
       "         [17, 37, 48],\n",
       "         [22, 42, 53]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[30, 53, 61],\n",
       "         [38, 61, 69],\n",
       "         [35, 58, 66],\n",
       "         ...,\n",
       "         [21, 37, 50],\n",
       "         [26, 42, 55],\n",
       "         [24, 40, 53]],\n",
       " \n",
       "        [[31, 54, 62],\n",
       "         [35, 58, 66],\n",
       "         [32, 55, 63],\n",
       "         ...,\n",
       "         [23, 39, 52],\n",
       "         [23, 39, 52],\n",
       "         [22, 38, 51]],\n",
       " \n",
       "        [[38, 61, 69],\n",
       "         [34, 57, 65],\n",
       "         [30, 53, 61],\n",
       "         ...,\n",
       "         [23, 39, 52],\n",
       "         [18, 34, 47],\n",
       "         [13, 29, 42]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/c96411eb-2cb38da0-10e2-11ee-ba5f-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.4638900756835938, 'inference': 112.4119758605957, 'postprocess': 0.4830360412597656},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[173, 193, 204],\n",
       "         [177, 197, 208],\n",
       "         [178, 198, 209],\n",
       "         ...,\n",
       "         [146, 161, 164],\n",
       "         [145, 160, 162],\n",
       "         [146, 161, 163]],\n",
       " \n",
       "        [[176, 196, 207],\n",
       "         [179, 199, 210],\n",
       "         [178, 198, 209],\n",
       "         ...,\n",
       "         [145, 160, 163],\n",
       "         [144, 159, 161],\n",
       "         [144, 159, 161]],\n",
       " \n",
       "        [[177, 198, 206],\n",
       "         [178, 199, 207],\n",
       "         [178, 199, 207],\n",
       "         ...,\n",
       "         [145, 160, 163],\n",
       "         [144, 159, 161],\n",
       "         [143, 158, 160]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 55,  61,  80],\n",
       "         [ 58,  64,  83],\n",
       "         [ 53,  59,  78],\n",
       "         ...,\n",
       "         [ 44,  38,  39],\n",
       "         [ 57,  53,  52],\n",
       "         [ 67,  63,  62]],\n",
       " \n",
       "        [[ 50,  56,  75],\n",
       "         [ 55,  61,  80],\n",
       "         [ 52,  58,  77],\n",
       "         ...,\n",
       "         [ 34,  28,  29],\n",
       "         [ 46,  42,  41],\n",
       "         [ 55,  53,  52]],\n",
       " \n",
       "        [[ 51,  57,  76],\n",
       "         [ 54,  60,  79],\n",
       "         [ 49,  55,  74],\n",
       "         ...,\n",
       "         [ 36,  31,  32],\n",
       "         [ 53,  48,  49],\n",
       "         [ 55,  53,  52]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/c9ebfc2e-9a550dba-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.352071762084961, 'inference': 114.0282154083252, 'postprocess': 0.4992485046386719},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[174, 191, 200],\n",
       "         [176, 193, 202],\n",
       "         [175, 192, 201],\n",
       "         ...,\n",
       "         [146, 163, 166],\n",
       "         [147, 164, 167],\n",
       "         [149, 166, 169]],\n",
       " \n",
       "        [[171, 188, 197],\n",
       "         [173, 190, 199],\n",
       "         [174, 191, 200],\n",
       "         ...,\n",
       "         [147, 164, 167],\n",
       "         [148, 165, 168],\n",
       "         [149, 166, 169]],\n",
       " \n",
       "        [[175, 192, 201],\n",
       "         [175, 192, 201],\n",
       "         [173, 190, 199],\n",
       "         ...,\n",
       "         [149, 166, 169],\n",
       "         [150, 167, 170],\n",
       "         [150, 167, 170]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 56,  60,  85],\n",
       "         [ 55,  59,  84],\n",
       "         [ 51,  55,  79],\n",
       "         ...,\n",
       "         [ 34,  25,  28],\n",
       "         [ 39,  31,  32],\n",
       "         [ 50,  42,  43]],\n",
       " \n",
       "        [[ 58,  62,  87],\n",
       "         [ 55,  59,  84],\n",
       "         [ 56,  58,  82],\n",
       "         ...,\n",
       "         [ 35,  26,  29],\n",
       "         [ 31,  22,  25],\n",
       "         [ 48,  39,  42]],\n",
       " \n",
       "        [[ 60,  64,  89],\n",
       "         [ 59,  63,  88],\n",
       "         [ 60,  62,  86],\n",
       "         ...,\n",
       "         [ 27,  18,  21],\n",
       "         [ 29,  20,  23],\n",
       "         [ 40,  31,  34]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/cdec6dd7-8f4a2162-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.5780925750732422, 'inference': 125.27275085449219, 'postprocess': 0.8230209350585938},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[176, 189, 197],\n",
       "         [174, 187, 195],\n",
       "         [171, 184, 192],\n",
       "         ...,\n",
       "         [146, 158, 162],\n",
       "         [142, 154, 158],\n",
       "         [146, 158, 162]],\n",
       " \n",
       "        [[173, 186, 194],\n",
       "         [176, 189, 197],\n",
       "         [172, 185, 193],\n",
       "         ...,\n",
       "         [145, 157, 161],\n",
       "         [140, 152, 156],\n",
       "         [139, 151, 155]],\n",
       " \n",
       "        [[174, 187, 195],\n",
       "         [177, 190, 198],\n",
       "         [174, 187, 195],\n",
       "         ...,\n",
       "         [140, 155, 158],\n",
       "         [140, 155, 158],\n",
       "         [138, 153, 156]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 58,  59,  79],\n",
       "         [ 60,  61,  81],\n",
       "         [ 57,  58,  78],\n",
       "         ...,\n",
       "         [ 33,  27,  38],\n",
       "         [ 40,  31,  41],\n",
       "         [ 44,  35,  45]],\n",
       " \n",
       "        [[ 56,  57,  77],\n",
       "         [ 57,  58,  78],\n",
       "         [ 58,  59,  79],\n",
       "         ...,\n",
       "         [ 34,  28,  39],\n",
       "         [ 34,  25,  35],\n",
       "         [ 40,  31,  41]],\n",
       " \n",
       "        [[ 56,  57,  77],\n",
       "         [ 53,  54,  74],\n",
       "         [ 58,  59,  79],\n",
       "         ...,\n",
       "         [ 21,  15,  26],\n",
       "         [ 35,  26,  36],\n",
       "         [ 40,  31,  41]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/d17d0693-95ce0eb8-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.691030502319336, 'inference': 325.5910873413086, 'postprocess': 0.6959438323974609},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         ...,\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234]],\n",
       " \n",
       "        [[246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         ...,\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234]],\n",
       " \n",
       "        [[246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         [246, 247, 245],\n",
       "         ...,\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234],\n",
       "         [224, 231, 234]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[213, 212, 214],\n",
       "         [213, 212, 214],\n",
       "         [213, 212, 214],\n",
       "         ...,\n",
       "         [ 74,  78,  89],\n",
       "         [ 73,  77,  88],\n",
       "         [ 72,  76,  87]],\n",
       " \n",
       "        [[214, 213, 215],\n",
       "         [213, 212, 214],\n",
       "         [212, 211, 213],\n",
       "         ...,\n",
       "         [ 74,  78,  89],\n",
       "         [ 73,  77,  88],\n",
       "         [ 72,  76,  87]],\n",
       " \n",
       "        [[214, 213, 215],\n",
       "         [213, 212, 214],\n",
       "         [212, 211, 213],\n",
       "         ...,\n",
       "         [ 74,  78,  89],\n",
       "         [ 73,  77,  88],\n",
       "         [ 72,  76,  87]]], dtype=uint8)\n",
       " orig_shape: (896, 2048)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/d3ab0bde-0308a25f-fd01-4ccc-9871-a948fddfe77d.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.105236053466797, 'inference': 102.15973854064941, 'postprocess': 0.5800724029541016},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[175, 194, 209],\n",
       "         [175, 194, 209],\n",
       "         [177, 196, 211],\n",
       "         ...,\n",
       "         [137, 156, 161],\n",
       "         [136, 155, 160],\n",
       "         [140, 159, 164]],\n",
       " \n",
       "        [[171, 190, 205],\n",
       "         [174, 193, 208],\n",
       "         [176, 195, 210],\n",
       "         ...,\n",
       "         [138, 157, 162],\n",
       "         [137, 156, 161],\n",
       "         [138, 157, 162]],\n",
       " \n",
       "        [[171, 190, 205],\n",
       "         [175, 194, 209],\n",
       "         [175, 194, 209],\n",
       "         ...,\n",
       "         [137, 156, 161],\n",
       "         [138, 157, 162],\n",
       "         [137, 156, 161]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 56,  63,  88],\n",
       "         [ 60,  67,  92],\n",
       "         [ 61,  68,  93],\n",
       "         ...,\n",
       "         [ 39,  35,  46],\n",
       "         [ 44,  43,  53],\n",
       "         [ 48,  47,  57]],\n",
       " \n",
       "        [[ 55,  62,  87],\n",
       "         [ 59,  66,  91],\n",
       "         [ 57,  64,  89],\n",
       "         ...,\n",
       "         [ 34,  30,  41],\n",
       "         [ 38,  37,  47],\n",
       "         [ 43,  42,  52]],\n",
       " \n",
       "        [[ 60,  67,  92],\n",
       "         [ 62,  69,  94],\n",
       "         [ 56,  63,  88],\n",
       "         ...,\n",
       "         [ 35,  31,  42],\n",
       "         [ 39,  38,  48],\n",
       "         [ 44,  43,  53]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/d578e605-94322ba2-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.7821788787841797, 'inference': 127.26688385009766, 'postprocess': 0.6692409515380859},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[159, 244, 240],\n",
       "         [159, 244, 240],\n",
       "         [159, 244, 240],\n",
       "         ...,\n",
       "         [183, 187, 175],\n",
       "         [184, 188, 176],\n",
       "         [185, 189, 177]],\n",
       " \n",
       "        [[160, 245, 241],\n",
       "         [160, 245, 241],\n",
       "         [160, 245, 241],\n",
       "         ...,\n",
       "         [184, 188, 176],\n",
       "         [185, 189, 177],\n",
       "         [185, 189, 177]],\n",
       " \n",
       "        [[161, 246, 242],\n",
       "         [161, 246, 242],\n",
       "         [161, 246, 242],\n",
       "         ...,\n",
       "         [184, 188, 176],\n",
       "         [185, 189, 177],\n",
       "         [186, 190, 178]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[147, 163, 179],\n",
       "         [146, 162, 178],\n",
       "         [146, 162, 178],\n",
       "         ...,\n",
       "         [ 33,  35,  36],\n",
       "         [ 32,  34,  35],\n",
       "         [ 31,  33,  34]],\n",
       " \n",
       "        [[147, 163, 179],\n",
       "         [147, 163, 179],\n",
       "         [146, 162, 178],\n",
       "         ...,\n",
       "         [ 35,  37,  38],\n",
       "         [ 32,  36,  37],\n",
       "         [ 34,  36,  37]],\n",
       " \n",
       "        [[149, 165, 178],\n",
       "         [148, 164, 177],\n",
       "         [148, 164, 177],\n",
       "         ...,\n",
       "         [ 36,  39,  43],\n",
       "         [ 34,  39,  42],\n",
       "         [ 35,  38,  42]]], dtype=uint8)\n",
       " orig_shape: (829, 1365)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/d7e84f41-92ce080a-39db-4ceb-85b5-f30997cc16f0.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.6731491088867188, 'inference': 170.9887981414795, 'postprocess': 0.7090568542480469},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[174, 191, 200],\n",
       "         [176, 193, 202],\n",
       "         [176, 193, 202],\n",
       "         ...,\n",
       "         [192, 214, 212],\n",
       "         [194, 216, 214],\n",
       "         [194, 216, 214]],\n",
       " \n",
       "        [[171, 188, 197],\n",
       "         [173, 190, 199],\n",
       "         [174, 191, 200],\n",
       "         ...,\n",
       "         [191, 213, 211],\n",
       "         [193, 215, 213],\n",
       "         [193, 215, 213]],\n",
       " \n",
       "        [[173, 190, 199],\n",
       "         [174, 191, 200],\n",
       "         [173, 190, 199],\n",
       "         ...,\n",
       "         [191, 213, 211],\n",
       "         [192, 214, 212],\n",
       "         [191, 213, 211]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 68,  75,  95],\n",
       "         [ 65,  72,  92],\n",
       "         [ 63,  70,  89],\n",
       "         ...,\n",
       "         [ 48,  41,  46],\n",
       "         [ 50,  40,  46],\n",
       "         [ 58,  48,  54]],\n",
       " \n",
       "        [[ 70,  76,  95],\n",
       "         [ 65,  71,  90],\n",
       "         [ 64,  70,  89],\n",
       "         ...,\n",
       "         [ 45,  38,  43],\n",
       "         [ 36,  26,  32],\n",
       "         [ 51,  41,  47]],\n",
       " \n",
       "        [[ 70,  76,  95],\n",
       "         [ 71,  77,  96],\n",
       "         [ 63,  69,  88],\n",
       "         ...,\n",
       "         [ 33,  26,  31],\n",
       "         [ 20,  10,  16],\n",
       "         [ 44,  34,  40]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/d904e901-9197fa70-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.0439624786376953, 'inference': 125.30398368835449, 'postprocess': 0.6680488586425781},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[179, 198, 205],\n",
       "         [177, 196, 203],\n",
       "         [176, 195, 202],\n",
       "         ...,\n",
       "         [141, 158, 161],\n",
       "         [143, 159, 165],\n",
       "         [144, 160, 166]],\n",
       " \n",
       "        [[179, 198, 205],\n",
       "         [178, 197, 204],\n",
       "         [177, 196, 203],\n",
       "         ...,\n",
       "         [143, 160, 163],\n",
       "         [144, 160, 166],\n",
       "         [142, 158, 164]],\n",
       " \n",
       "        [[177, 196, 203],\n",
       "         [177, 196, 203],\n",
       "         [177, 196, 203],\n",
       "         ...,\n",
       "         [150, 165, 168],\n",
       "         [146, 161, 164],\n",
       "         [144, 159, 162]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 63,  70,  90],\n",
       "         [ 61,  68,  88],\n",
       "         [ 65,  70,  91],\n",
       "         ...,\n",
       "         [ 47,  43,  48],\n",
       "         [ 44,  40,  45],\n",
       "         [ 51,  47,  52]],\n",
       " \n",
       "        [[ 65,  72,  92],\n",
       "         [ 61,  68,  88],\n",
       "         [ 63,  70,  89],\n",
       "         ...,\n",
       "         [ 44,  40,  46],\n",
       "         [ 43,  39,  45],\n",
       "         [ 47,  43,  49]],\n",
       " \n",
       "        [[ 61,  68,  88],\n",
       "         [ 58,  65,  85],\n",
       "         [ 63,  70,  89],\n",
       "         ...,\n",
       "         [ 42,  38,  44],\n",
       "         [ 42,  38,  44],\n",
       "         [ 45,  41,  47]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/dc07c56d-9a03b168-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.062082290649414, 'inference': 125.88906288146973, 'postprocess': 0.7081031799316406},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[176, 194, 201],\n",
       "         [170, 188, 195],\n",
       "         [173, 191, 198],\n",
       "         ...,\n",
       "         [151, 166, 168],\n",
       "         [149, 164, 166],\n",
       "         [149, 164, 166]],\n",
       " \n",
       "        [[173, 191, 198],\n",
       "         [180, 198, 205],\n",
       "         [177, 195, 202],\n",
       "         ...,\n",
       "         [148, 163, 165],\n",
       "         [149, 164, 166],\n",
       "         [150, 165, 167]],\n",
       " \n",
       "        [[174, 192, 199],\n",
       "         [180, 198, 205],\n",
       "         [176, 194, 201],\n",
       "         ...,\n",
       "         [149, 164, 166],\n",
       "         [149, 164, 166],\n",
       "         [148, 163, 165]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 65,  70,  95],\n",
       "         [ 62,  67,  92],\n",
       "         [ 57,  63,  86],\n",
       "         ...,\n",
       "         [ 40,  39,  41],\n",
       "         [ 41,  39,  39],\n",
       "         [ 56,  54,  54]],\n",
       " \n",
       "        [[ 64,  69,  94],\n",
       "         [ 63,  68,  93],\n",
       "         [ 61,  67,  90],\n",
       "         ...,\n",
       "         [ 43,  42,  44],\n",
       "         [ 41,  38,  40],\n",
       "         [ 48,  45,  47]],\n",
       " \n",
       "        [[ 66,  71,  96],\n",
       "         [ 61,  66,  91],\n",
       "         [ 64,  70,  93],\n",
       "         ...,\n",
       "         [ 31,  30,  32],\n",
       "         [ 38,  35,  37],\n",
       "         [ 39,  36,  38]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/e4fa758d-99611278-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 2.045869827270508, 'inference': 115.34905433654785, 'postprocess': 0.7729530334472656},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " orig_img: array([[[173, 194, 209],\n",
       "         [173, 194, 209],\n",
       "         [173, 194, 209],\n",
       "         ...,\n",
       "         [140, 159, 166],\n",
       "         [141, 160, 167],\n",
       "         [140, 159, 166]],\n",
       " \n",
       "        [[172, 193, 208],\n",
       "         [172, 193, 208],\n",
       "         [172, 193, 208],\n",
       "         ...,\n",
       "         [138, 157, 164],\n",
       "         [138, 157, 164],\n",
       "         [138, 157, 164]],\n",
       " \n",
       "        [[174, 195, 210],\n",
       "         [174, 195, 210],\n",
       "         [174, 195, 210],\n",
       "         ...,\n",
       "         [138, 157, 164],\n",
       "         [140, 159, 166],\n",
       "         [141, 160, 167]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 48,  56,  85],\n",
       "         [ 49,  57,  86],\n",
       "         [ 48,  56,  85],\n",
       "         ...,\n",
       "         [ 39,  36,  38],\n",
       "         [ 27,  26,  30],\n",
       "         [ 41,  40,  44]],\n",
       " \n",
       "        [[ 49,  57,  86],\n",
       "         [ 49,  57,  86],\n",
       "         [ 47,  55,  84],\n",
       "         ...,\n",
       "         [ 30,  27,  29],\n",
       "         [ 26,  25,  29],\n",
       "         [ 33,  32,  36]],\n",
       " \n",
       "        [[ 52,  60,  89],\n",
       "         [ 50,  58,  87],\n",
       "         [ 47,  55,  84],\n",
       "         ...,\n",
       "         [ 21,  18,  20],\n",
       "         [ 21,  20,  24],\n",
       "         [ 35,  34,  38]]], dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/Users/rei/ml/araya/yolov8_test/data/test/images/e723a8b2-923ad2ea-31cc-11ee-a456-e68a90e86460.jpg'\n",
       " probs: None\n",
       " save_dir: None\n",
       " speed: {'preprocess': 1.7981529235839844, 'inference': 129.55784797668457, 'postprocess': 0.7190704345703125}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"best.pt\")\n",
    "test_img = 'data/test/images'\n",
    "model.predict(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING ⚠️ 'ultralytics.yolo.cfg' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.cfg' instead.\n",
      "Ultralytics YOLOv8.0.147 🚀 Python-3.11.4 torch-2.0.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "\n",
      "image 1/15 /Users/rei/ml/araya/yolov8_test/data/test/images/bd29bc18-e385a116-1fed-4fd3-acba-29db1659da3a.jpg: 352x640 4 persons, 159.3ms\n",
      "image 2/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c1f4b19e-3c3c8c68-10e2-11ee-ba5f-e68a90e86460.jpg: 384x640 2 persons, 145.2ms\n",
      "image 3/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c61c9f56-98119b72-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 119.3ms\n",
      "image 4/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c6fd24b8-d9ba4b72-0ac9-49ac-95a6-d4553bef8f46.jpg: 448x640 1 person, 116.6ms\n",
      "image 5/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c96411eb-2cb38da0-10e2-11ee-ba5f-e68a90e86460.jpg: 384x640 1 person, 133.8ms\n",
      "image 6/15 /Users/rei/ml/araya/yolov8_test/data/test/images/c9ebfc2e-9a550dba-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 197.3ms\n",
      "image 7/15 /Users/rei/ml/araya/yolov8_test/data/test/images/cdec6dd7-8f4a2162-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 355.9ms\n",
      "image 8/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d17d0693-95ce0eb8-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 110.2ms\n",
      "image 9/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d3ab0bde-0308a25f-fd01-4ccc-9871-a948fddfe77d.jpg: 288x640 1 person, 139.9ms\n",
      "image 10/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d578e605-94322ba2-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 163.1ms\n",
      "image 11/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d7e84f41-92ce080a-39db-4ceb-85b5-f30997cc16f0.jpg: 416x640 1 person, 270.7ms\n",
      "image 12/15 /Users/rei/ml/araya/yolov8_test/data/test/images/d904e901-9197fa70-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 122.3ms\n",
      "image 13/15 /Users/rei/ml/araya/yolov8_test/data/test/images/dc07c56d-9a03b168-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 197.6ms\n",
      "image 14/15 /Users/rei/ml/araya/yolov8_test/data/test/images/e4fa758d-99611278-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 116.5ms\n",
      "image 15/15 /Users/rei/ml/araya/yolov8_test/data/test/images/e723a8b2-923ad2ea-31cc-11ee-a456-e68a90e86460.jpg: 384x640 1 person, 144.4ms\n",
      "Speed: 2.4ms preprocess, 166.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1m/Users/rei/runs/detect/predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_img = '/Users/rei/ml/araya/yolov8_test/data/test/images'\n",
    "!yolo task=detect mode=predict model='best.pt' conf=0.25 source='/Users/rei/ml/araya/yolov8_test/data/test/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "model.export(format='saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.147 🚀 Python-3.11.4 torch-2.0.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.13.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.33...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.5s, saved as 'best.onnx' (11.6 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m running 'onnx2tf -i \"best.onnx\" -o \"best_saved_model\" -nuo --non_verbose'\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 9.6s, saved as 'best_saved_model' (29.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m starting export with tensorflow 2.13.0...\n",
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m export success ✅ 0.4s, saved as 'best.pb' (11.7 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorflowjs'] not found, attempting AutoUpdate...\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter 1.0.0 requires qtconsole, which is not installed.\n",
      "statsmodels 0.14.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "jupyterlab-server 2.24.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "scikit-image 0.21.0 requires packaging>=21, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting tensorflowjs\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/78/77/f9a83027eca63ac777daf3c133a53f77c47139a25d236629d5634b0e2025/tensorflowjs-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting flax<0.6.3,>=0.6.2 (from tensorflowjs)\n",
      "  Downloading flax-0.6.2-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.9/189.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib_resources>=5.9.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflowjs) (6.0.0)\n",
      "Collecting jax>=0.3.16 (from tensorflowjs)\n",
      "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow<3,>=2.12.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflowjs) (2.13.0)\n",
      "Collecting tensorflow-decision-forests>=1.3.0 (from tensorflowjs)\n",
      "  Obtaining dependency information for tensorflow-decision-forests>=1.3.0 from https://files.pythonhosted.org/packages/3e/8b/48eccda39c838a1b59c115230be634e54d5a013ab90f8df4f39daaad7946/tensorflow_decision_forests-1.5.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_decision_forests-1.5.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflowjs) (1.16.0)\n",
      "Collecting tensorflow-hub>=0.13.0 (from tensorflowjs)\n",
      "  Obtaining dependency information for tensorflow-hub>=0.13.0 from https://files.pythonhosted.org/packages/30/78/9d5292a2b616901bdb075bbf0c777b293f4140bb48108ac2b33fd716c2eb/tensorflow_hub-0.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_hub-0.14.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting packaging~=20.9 (from tensorflowjs)\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (3.7.2)\n",
      "Collecting msgpack (from flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Downloading msgpack-1.0.5-cp311-cp311-macosx_11_0_arm64.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m293.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optax (from flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for optax from https://files.pythonhosted.org/packages/13/71/787cc24c4b606f3bb9f1d14957ebd7cb9e4234f6d59081721230b2032196/optax-0.1.7-py3-none-any.whl.metadata\n",
      "  Downloading optax-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tensorstore (from flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for tensorstore from https://files.pythonhosted.org/packages/82/65/b272cdc7ac2f079fc73d51bfb8a52c5d6f007faf5aa0a236c9fd63a8d7fd/tensorstore-0.1.40-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tensorstore-0.1.40-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: rich>=11.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (13.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (4.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (6.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.3.16->tensorflowjs)\n",
      "  Obtaining dependency information for ml-dtypes>=0.2.0 from https://files.pythonhosted.org/packages/15/da/43bee505963da0c730ee50e951c604bfdb90d4cccc9c0044c946b10e68a7/ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from jax>=0.3.16->tensorflowjs) (1.11.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (16.0.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (68.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.56.2)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.1)\n",
      "Requirement already satisfied: pandas in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (2.0.3)\n",
      "Requirement already satisfied: wheel in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (0.41.0)\n",
      "Collecting wurlitzer (from tensorflow-decision-forests>=1.3.0->tensorflowjs)\n",
      "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (2.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (4.41.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (2.8.2)\n",
      "Collecting chex>=0.1.5 (from optax->flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for chex>=0.1.5 from https://files.pythonhosted.org/packages/84/c9/a2182cbf8bc066de9433930c41e76b7f4e904f155c5881235dbb54f8148b/chex-0.1.82-py3-none-any.whl.metadata\n",
      "  Downloading chex-0.1.82-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting jaxlib>=0.1.37 (from optax->flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for jaxlib>=0.1.37 from https://files.pythonhosted.org/packages/5a/fc/c74fc6bb2f61ee21209c344d957754eadb155443cfdda4d2d5a54f81bb7f/jaxlib-0.4.14-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading jaxlib-0.4.14-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from pandas->tensorflow-decision-forests>=1.3.0->tensorflowjs) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from pandas->tensorflow-decision-forests>=1.3.0->tensorflowjs) (2023.3)\n",
      "INFO: pip is looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting chex>=0.1.5 (from optax->flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for chex>=0.1.5 from https://files.pythonhosted.org/packages/f4/c2/e66bf06bff9d2fad600b0daebf54ecb5d7e2c40eae25968773e3ce292814/chex-0.1.81-py3-none-any.whl.metadata\n",
      "  Downloading chex-0.1.81-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting dm-tree>=0.1.5 (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chex>=0.1.5 (from optax->flax<0.6.3,>=0.6.2->tensorflowjs)\n",
      "  Downloading chex-0.1.7-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m168.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/rei/miniforge3/envs/aryenv/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.2)\n",
      "Downloading tensorflowjs-4.10.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m379.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_decision_forests-1.5.0-cp311-cp311-macosx_12_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_hub-0.14.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.3/90.3 kB\u001b[0m \u001b[31m174.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading optax-0.1.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorstore-0.1.40-cp311-cp311-macosx_11_0_arm64.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.14-cp311-cp311-macosx_11_0_arm64.whl (63.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535362 sha256=38f840eb4e23b2f120fcfb4c248c5f684252cb22f3d3f5b5793d6808a237d139\n",
      "  Stored in directory: /private/var/folders/y0/hktw1r390c7_b4jqk2w1gj540000gn/T/pip-ephem-wheel-cache-sazyslm0/wheels/c4/8d/5d/66b1fbb551b0c3a21696015b7339b8241ebfa128bb9145febd\n",
      "Successfully built jax\n",
      "Installing collected packages: msgpack, dm-tree, wurlitzer, tensorstore, tensorflow-hub, packaging, ml-dtypes, jaxlib, jax, chex, optax, flax, tensorflow-decision-forests, tensorflowjs\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed chex-0.1.7 dm-tree-0.1.8 flax-0.6.2 jax-0.4.14 jaxlib-0.4.14 ml-dtypes-0.2.0 msgpack-1.0.5 optax-0.1.7 packaging-20.9 tensorflow-decision-forests-1.5.0 tensorflow-hub-0.14.0 tensorflowjs-4.10.0 tensorstore-0.1.40 wurlitzer-3.0.3\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 20.7s, installed 1 package: ['tensorflowjs']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow.js:\u001b[0m starting export with tensorflowjs 4.10.0...\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow.js:\u001b[0m output node names: Identity:0\n",
      "\u001b[34m\u001b[1mTensorFlow.js:\u001b[0m running 'tensorflowjs_converter --input_format=tf_frozen_model --output_node_names=Identity:0 \"best.pb\" \"best_web_model\"'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight 1319 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1321 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_19/ones_like with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1313 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1315 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_16/StridedSlice/strides with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1323 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1325 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_18/ones_like with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1233 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1235 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_14/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1203 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1205 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_12/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1173 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1175 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_10/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1161 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1163 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_11/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1149 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1151 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_8/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1137 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1139 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_9/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1191 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1193 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_13/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1117 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1119 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_6/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1087 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1089 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_4/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1049 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1051 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_2/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1011 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1013 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 999 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1001 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_1/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1029 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1031 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_3/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1067 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1069 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_5/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1105 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1107 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_7/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1221 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1223 with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_15/ones_like with shape (4,) and dtype int64 was auto converted to the type int32\n",
      "weight 1333 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight 1335 with shape (3,) and dtype int64 was auto converted to the type int32\n",
      "weight PartitionedCall/model_2/tf.strided_slice_17/StridedSlice/strides with shape (3,) and dtype int64 was auto converted to the type int32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow.js:\u001b[0m export success ✅ 31.6s, saved as 'best_web_model' (11.8 MB)\n",
      "\n",
      "Export complete (42.2s)\n",
      "Results saved to \u001b[1m/Users/rei/ml/araya/yolov8_test\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best_web_model imgsz=640 \n",
      "Validate:        yolo val task=detect model=best_web_model imgsz=640 data=None \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'best_web_model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "model.export(format='tfjs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aryenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
